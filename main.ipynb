{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zrlTQ4RsXU-V"
      },
      "source": [
        "# NLP TP2 - Victor Henrique Silva Ribeiro\n",
        "\n",
        "\n",
        "## Introdução\n",
        "Nesse trabalho, irei utilizar o modelo pré-treinado `bert-base-portuguese-cased` para a tarefa downstream de POS tagging. Para isso, ultilizo o dataset `macmorpho`, que é um dataset de POS tagging para o português.\n",
        "\n",
        "Primeiramente importo as bibliotecas necessárias para o trabalho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wb-cNA1mXU-W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: requests in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: six in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: torch in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.6.0) (2.1.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.6.0) (1.26.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.6.0) (2023.11.17)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->torchtext==0.6.0) (2023.10.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->torchtext==0.6.0) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->torchtext==0.6.0) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.35.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: requests in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: filelock in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.15.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2023.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.26.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: pandas in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.1.3)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (14.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: xxhash in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vitão\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vitão\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install torchtext==0.6.0\n",
        "%pip install transformers\n",
        "%pip install numpy\n",
        "%pip install torch\n",
        "%pip install datasets\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Example, Dataset\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import functools\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj4DBdJ4XU-e"
      },
      "source": [
        "O primeiro passo é importar o tokenizador em português utilizando a biblioteca `transformers` do HuggingFace. É importante lembrar que é necessário utilizar em nossos inputs os tokens de começo de frase, token desconhecido e padding que foram utilizados no treinamento do `BERT`. Além disso precisamos truncar nossos inputs para o tamanho máximo de tokens que o `BERT` suporta, que é 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nS2XMkfxXU-f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer_config.json: 100%|██████████| 43.0/43.0 [00:00<?, ?B/s]\n",
            "c:\\Users\\Vitão\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Vitão\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "vocab.txt: 100%|██████████| 210k/210k [00:00<00:00, 3.17MB/s]\n",
            "added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<?, ?B/s]\n",
            "special_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
            "config.json: 100%|██████████| 647/647 [00:00<00:00, 644kB/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "init_token = tokenizer.cls_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KD_ChHl0XU-v"
      },
      "source": [
        "Agora definimos como os inputs e labels serão pré-processados para o formato que o `BERT` espera. Todo o processo é feito utilizando tensores `PyTorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G_Xjhu3jXU-v"
      },
      "outputs": [],
      "source": [
        "def inputProcessor(tokens, tokenizer, max_input_length):\n",
        "    tokens = tokens[:max_input_length-1]\n",
        "    tokens = [tokenizer.convert_tokens_to_ids(token) \n",
        "              if token in tokenizer.vocab \n",
        "              else tokenizer.convert_tokens_to_ids('<unk>') \n",
        "              for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "def labelProcessor(tokens, max_input_length):\n",
        "    tokens = tokens[:max_input_length-1]\n",
        "    return tokens\n",
        "\n",
        "text_preprocessor = functools.partial(inputProcessor,\n",
        "                                      tokenizer = tokenizer,\n",
        "                                      max_input_length = max_input_length)\n",
        "\n",
        "tag_preprocessor = functools.partial(labelProcessor,\n",
        "                                     max_input_length = max_input_length)\n",
        "\n",
        "TEXT = data.Field(use_vocab = False,\n",
        "                  lower = True,\n",
        "                  preprocessing = text_preprocessor,\n",
        "                  init_token = init_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "UD_TAGS = data.Field(unk_token = None,\n",
        "                     init_token = '<pad>',\n",
        "                     preprocessing = tag_preprocessor)\n",
        "\n",
        "fields = ((\"tokens\", TEXT), (\"pos_tags\", UD_TAGS))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xJotJpLIXU_B"
      },
      "source": [
        "Aqui importamos o dataset `macmorpho` utilizando a biblioteca `datasets` do HuggingFace. O dataset é dividido em treino, validação e teste. O dataset de treino é utilizado para treinar o modelo, o de validação é utilizado para escolher o melhor modelo e o de teste é utilizado para avaliar o modelo final.\n",
        "\n",
        "Depois de definidas as divisões transformo elas em tensores `PyTorch` usando os procedimentos definidos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gn7OmDceXU_C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 6.33k/6.33k [00:00<?, ?B/s]\n",
            "Downloading metadata: 100%|██████████| 3.36k/3.36k [00:00<?, ?B/s]\n",
            "Downloading readme: 100%|██████████| 6.49k/6.49k [00:00<?, ?B/s]\n",
            "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 7.46MB/s]\n",
            "Generating train split: 100%|██████████| 37948/37948 [00:06<00:00, 5746.80 examples/s]\n",
            "Generating test split: 100%|██████████| 9987/9987 [00:01<00:00, 6620.87 examples/s]\n",
            "Generating validation split: 100%|██████████| 1997/1997 [00:00<00:00, 5972.10 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37948\n",
            "1997\n",
            "9987\n"
          ]
        }
      ],
      "source": [
        "def toPytorchDataset(dataset, train_set=None):\n",
        "    dataset = [(example['tokens'], example['pos_tags']) for example in dataset]\n",
        "\n",
        "    examples = [Example.fromlist([text, tags], fields=[('text', TEXT), ('udtags', UD_TAGS)]) for text, tags in dataset]\n",
        "    dataset = Dataset(examples, fields=[('text', TEXT), ('udtags', UD_TAGS)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset('mac_morpho')\n",
        "train_data_raw = dataset['train']\n",
        "valid_data_raw = dataset['validation']\n",
        "test_data_raw = dataset['test']\n",
        "\n",
        "train_data = toPytorchDataset(train_data_raw)\n",
        "valid_data = toPytorchDataset(valid_data_raw, train_set=train_data_raw)\n",
        "test_data = toPytorchDataset(test_data_raw, train_set=train_data_raw)\n",
        "\n",
        "print(len(train_data.examples))\n",
        "print(len(valid_data.examples))\n",
        "print(len(test_data.examples))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J3HiRqq8XU_H"
      },
      "source": [
        "É necessário construir o vocabulário para as tags, para que elas possam ser indexadas durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klpqOEHJXU_I",
        "outputId": "21035e0f-1143-4b35-a324-4198cdff9f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(None, {'<pad>': 0, 14: 1, 24: 2, 19: 3, 3: 4, 15: 5, 25: 6, 9: 7, 12: 8, 23: 9, 5: 10, 21: 11, 7: 12, 8: 13, 10: 14, 11: 15, 6: 16, 16: 17, 18: 18, 22: 19, 0: 20, 13: 21, 4: 22, 17: 23, 1: 24, 2: 25, 20: 26})\n"
          ]
        }
      ],
      "source": [
        "UD_TAGS.build_vocab(train_data)\n",
        "print(UD_TAGS.vocab.stoi)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gROHWG7tXU_P"
      },
      "source": [
        "Importando o modelo pré-treinado `bert-base-portuguese-cased` e adicionando a camada linear no final para classificar as tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0bYCKE0xXU_P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pytorch_model.bin: 100%|██████████| 438M/438M [00:09<00:00, 47.5MB/s] \n"
          ]
        }
      ],
      "source": [
        "class BERTPoSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 output_dim, \n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        text = text.permute(1, 0)\n",
        "\n",
        "        embedded = self.dropout(self.bert(text)[0])\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        predictions = self.fc(self.dropout(embedded))\n",
        "        \n",
        "        return predictions\n",
        "    \n",
        "bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTPoSTagger(bert,\n",
        "                      OUTPUT_DIM, \n",
        "                      DROPOUT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora defino o procedimento de treinamento da camada linear. Todo o processo será realizado na CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BgIEyEkpXU_M"
      },
      "outputs": [],
      "source": [
        "def sort_key(example):\n",
        "    return len(example.text)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "device = torch.device('cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort_key = sort_key)\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i5G_0OckXU_o"
      },
      "source": [
        "Definindo as funções de treino e avaliação do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mk9vFsMuXU_p"
      },
      "outputs": [],
      "source": [
        "def getAccuracy(preds, y, tag_pad_idx):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        try:\n",
        "            text = batch.text\n",
        "            tags = batch.udtags\n",
        "                    \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            predictions = model(text)\n",
        "            \n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            \n",
        "            loss = criterion(predictions, tags)\n",
        "            acc = getAccuracy(predictions, tags, tag_pad_idx)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "            \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    confusion_matrix = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            try:\n",
        "                text = batch.text\n",
        "                tags = batch.udtags\n",
        "\n",
        "                predictions = model(text)\n",
        "\n",
        "                predictions = predictions.view(-1, predictions.shape[-1])\n",
        "                tags = tags.view(-1)\n",
        "\n",
        "                loss = criterion(predictions, tags)\n",
        "\n",
        "                acc = getAccuracy(predictions, tags, tag_pad_idx)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "                # Update confusion matrix\n",
        "                max_preds = predictions.argmax(dim=1)\n",
        "                for pred, actual in zip(max_preds, tags):\n",
        "                    pred_tag = UD_TAGS.vocab.itos[pred.item()]\n",
        "                    actual_tag = UD_TAGS.vocab.itos[actual.item()]\n",
        "                    confusion_matrix[actual_tag]['total'] += 1\n",
        "                    if pred_tag == actual_tag:\n",
        "                        confusion_matrix[actual_tag]['correct'] += 1\n",
        "\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    # Calculate accuracy for each tag\n",
        "    for tag, data in confusion_matrix.items():\n",
        "        accuracy = data['correct'] / data['total']\n",
        "        print(f\"Accuracy for tag {tag}: {accuracy}\")\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TSYdBkOrXU_y"
      },
      "source": [
        "Treinando o modelo, cada época levou cerca de 22 minutos para ser concluída."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F53iNXOXU_y",
        "outputId": "388280c3-2f14-413b-9b6e-047cf051f62d"
      },
      "outputs": [],
      "source": [
        "model_path = 'models/pos-tagging-model.pt'\n",
        "\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    print('Epoch: %02d' % (epoch+1))\n",
        "    print('\\tTrain Loss: %.3f | Train Acc: %.2f%%' % (train_loss, train_acc*100))\n",
        "    print('\\t Val. Loss: %.3f |  Val. Acc: %.2f%%' % (valid_loss, valid_acc*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yi8JGgQ_XU_1"
      },
      "source": [
        "Carregando o modelo com a melhor acurácia no dataset de validação e testando no dataset de teste. Obtendo uma acurácia de 93.29%. \n",
        "\n",
        "Com base nesses resultados podemos ver que a acurácia para as tags 24, 9 e 25 chegam perto de 99%. Essas tags representam pontuação, preposição-artigo e artigo, respectivamente.\n",
        "\n",
        "As tags 17, 20 e 4 tem a menor acurácia, certa de 70%. Essas tags representam preposição-pronome pessoal, preposição-advérbio e preposição-pronome substantivo, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6wyOs_XU_1",
        "outputId": "935315fa-d552-4194-d29a-006317e9ae32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for tag <pad>: 0.0\n",
            "Accuracy for tag 14: 0.9421761261014723\n",
            "Accuracy for tag 3: 0.8829066265060241\n",
            "Accuracy for tag 24: 0.997992863514719\n",
            "Accuracy for tag 19: 0.9713865354370655\n",
            "Accuracy for tag 23: 0.8883584282041865\n",
            "Accuracy for tag 9: 0.9881593110871906\n",
            "Accuracy for tag 12: 0.8860182370820668\n",
            "Accuracy for tag 18: 0.8901098901098901\n",
            "Accuracy for tag 25: 0.992368839427663\n",
            "Accuracy for tag 1: 0.7142857142857143\n",
            "Accuracy for tag 16: 0.8876834716017868\n",
            "Accuracy for tag 21: 0.817032967032967\n",
            "Accuracy for tag 8: 0.9578824217607488\n",
            "Accuracy for tag 15: 0.9799141733222076\n",
            "Accuracy for tag 6: 0.9429763560500696\n",
            "Accuracy for tag 10: 0.8944050433412135\n",
            "Accuracy for tag 7: 0.9252262888626525\n",
            "Accuracy for tag 17: 0.7936507936507936\n",
            "Accuracy for tag 22: 0.8682432432432432\n",
            "Accuracy for tag 5: 0.9792540278084308\n",
            "Accuracy for tag 20: 0.7096774193548387\n",
            "Accuracy for tag 4: 0.7115384615384616\n",
            "Accuracy for tag 11: 0.9658314350797267\n",
            "Accuracy for tag 0: 0.9870550161812298\n",
            "Accuracy for tag 13: 0.8347826086956521\n",
            "Accuracy for tag 2: 0.9482758620689655\n",
            "Test Loss: 0.214 | Test Acc: 93.29%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
        "print('Test Loss: %.3f | Test Acc: %.2f%%' % (test_loss, test_acc*100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "2 - Fine-tuning Pretrained Transformers for PoS Tagging.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
